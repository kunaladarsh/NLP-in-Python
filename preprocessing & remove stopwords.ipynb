{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ***Simple Language Processing***\n",
        "\n",
        "**Read text information, perform Preprocessing on the data, Remove Stopwords, Perform Tokenization, Classify the document words based on specific topics with Modelling.**\n",
        "\n",
        "Procedure:\n",
        "\n",
        "• Load Necessary Dependencies.\n",
        "\n",
        "• Load and Process the Datasets.\n",
        "\n",
        "• Inspection of the data.\n",
        "\n",
        "• Preprocess the Data.\n",
        "\n",
        "• Remove stopwords.\n",
        "\n",
        "• Perform Tokenization.\n",
        "\n",
        "• Model and display the text categories as Topics.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fuOS2KqqYOw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dNIJ1uk8Ov7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ebf5158-3bf2-4c3b-d94d-753af1a3a8d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n",
            "Collecting pip\n",
            "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-67.3.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (0.38.4)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.4\n",
            "    Uninstalling pip-22.0.4:\n",
            "      Successfully uninstalled pip-22.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-23.0.1 setuptools-67.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install -U pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U spacy"
      ],
      "metadata": {
        "id": "_aUXO0-MPmwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c962305-c626-44c8-96c3-10f561f67456"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (67.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.5.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U pip setuptools wheel\n"
      ],
      "metadata": {
        "id": "6i2pZUFdQrTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939efc4b-495e-4276-c4e3-718b90bfbc3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (23.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (67.3.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (0.38.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U spacy\n"
      ],
      "metadata": {
        "id": "diicM-rkVgTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533d1c70-7786-413a-ba20-7ba409b6238d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.7)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (67.3.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1nHaLYoVq3K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "AZh5OGZuSQEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d99f65-fae5-4446-de69-2f1c53693083"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "/usr/local/lib/python3.8/dist-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n"
      ],
      "metadata": {
        "id": "s7VdbTNNWGSI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = en_core_web_sm.load()\n"
      ],
      "metadata": {
        "id": "0wcq-uxuWJEY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is a sentence.\")\n"
      ],
      "metadata": {
        "id": "6xc3LEGBWLjI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([(w.text, w.pos_) for w in doc])\n"
      ],
      "metadata": {
        "id": "l7CeoWb9WN9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde2cff6-f12c-4646-fc21-ce8cb7bab084"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('sentence', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy_nlp = spacy.load('en_core_web_sm')    "
      ],
      "metadata": {
        "id": "SiZ__jAzWlMa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = spacy_nlp.Defaults.stop_words\n",
        "print('Stopwords in Spacy are:')\n",
        "print()\n",
        "print(len(stopwords))\n",
        "print(stopwords)\n"
      ],
      "metadata": {
        "id": "WU07Qk5CWrGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c4fba8-0b00-4584-96fb-04fb4667a5f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords in Spacy are:\n",
            "\n",
            "326\n",
            "{'besides', 'ten', 'least', 'what', 'became', 'twenty', 'something', 'by', 'less', 'mostly', 'much', 'them', 'each', 'none', 'doing', 'will', 'former', 'keep', 'these', 'are', 'us', 'whom', 'anyway', 'serious', 'last', 'hereupon', 'above', 'with', 'its', 'another', 'throughout', 'just', 'therein', 'now', 'here', 'however', 'down', 'nowhere', 'ca', 'cannot', 'so', 'please', 'an', 're', 'yourself', 'do', 'though', 'whereas', 'indeed', 'does', 'almost', 'amongst', 'anyone', 'already', 'while', 'all', 'yourselves', 'five', 'through', 'everything', 'seems', 'often', 'against', 'both', 'two', 'then', 'third', 'noone', 'my', 'can', 'or', 'becomes', 'the', '‘m', 'nothing', 'enough', 'ourselves', 'his', 'themselves', 'to', 'have', 'of', 'for', 'move', 'before', 'on', 'we', 'she', 'whenever', 'under', 'amount', \"'d\", 'as', 'were', 'either', 'which', 'behind', 'they', 'sixty', 'fifteen', 'if', 'take', 'since', 'up', 'side', 'somehow', 'towards', 'otherwise', 'over', 'six', 'own', 'full', 'one', 'say', 'yours', 'himself', 'next', 'had', 'sometimes', 'there', 'you', 'off', 'seemed', 'this', 'why', 'using', 'afterwards', 'across', 'whose', 'meanwhile', 'along', 'else', 'sometime', 'may', 'nevertheless', 'via', 'someone', 'back', 'thence', 'whence', '’s', 'many', 'only', 'being', 'too', 'first', 'n‘t', 'did', 'forty', 'among', 'anything', 'no', 'those', 'anywhere', 'really', 'without', 'about', 'same', '’d', 'made', '‘ll', 'name', 'in', 'formerly', '‘re', \"'m\", 'others', 'four', 'itself', 'regarding', 'more', 'elsewhere', 'few', 'even', 'whoever', 'beforehand', 'a', 'hereafter', \"n't\", 'herein', 'not', 'is', 'ours', 'because', '’ll', 'yet', 'could', 'everywhere', 'nor', 'wherever', 'at', 'neither', 'get', 'our', 'quite', 'done', 'should', 'although', 'it', 'hundred', 'also', 'becoming', 'other', 'whereupon', 'very', 'make', 'whole', 'has', 'eight', 'except', 'he', 'their', 'always', 'herself', '‘d', 'part', 'beyond', 'that', 'was', 'thereby', 'into', 'onto', 'how', 'hence', 'latterly', 'somewhere', 'become', 'mine', 'whither', 'n’t', 'hers', 'give', 'front', 'used', 'put', 'never', 'but', 'myself', 'three', 'most', 'therefore', 'call', 'whereby', 'ever', 'i', 'unless', \"'s\", '‘ve', 'alone', 'seem', 'still', 'fifty', 'be', 'during', 'where', 'due', 'between', \"'re\", 'him', 'namely', 'again', 'whereafter', '’re', 'whether', 'once', 'would', 'several', 'whatever', 'everyone', 'than', 'hereby', 'thereupon', 'some', \"'ll\", 'after', 'moreover', 'from', 'thereafter', 'toward', 'wherein', 'below', '‘s', 'well', 'upon', 'latter', 'see', 'must', 'your', 'various', 'beside', 'who', 'together', 'nine', 'every', 'per', 'anyhow', 'show', 'seeming', 'am', 'been', 'her', 'perhaps', '’ve', 'rather', 'eleven', 'twelve', 'any', 'empty', 'might', 'bottom', 'around', 'when', 'until', 'thru', 'thus', \"'ve\", 'nobody', 'top', 'within', 'out', '’m', 'and', 'such', 'go', 'further', 'me'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##in case you want to add your own stopword\n",
        "spacy_nlp.Defaults.stop_words.add(\"spacy\")\n"
      ],
      "metadata": {
        "id": "CySVkcvpWxPv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = spacy_nlp(\"we will show how to remove stopwords using spacy library \")\n"
      ],
      "metadata": {
        "id": "J-5bbDOGW-QW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##geting tokens\n",
        "\n",
        "doc = spacy_nlp(article)\n",
        "tokens = [token.text for token in doc]\n",
        "print('Original Article: %s' % (article))\n",
        "print()\n",
        "print('Tokens:',tokens)\n"
      ],
      "metadata": {
        "id": "ikuqZsvqXBl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd82ee32-22a8-4e90-a5f6-6c266d3858ee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Article: we will show how to remove stopwords using spacy library \n",
            "\n",
            "Tokens: ['we', 'will', 'show', 'how', 'to', 'remove', 'stopwords', 'using', 'spacy', 'library']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lst=[]\n",
        "for token in tokens:\n",
        "    if token.lower() not in stopwords:    #checking whether the word is not \n",
        "        lst.append(token)                    #present in the stopword list.\n",
        "print()        \n",
        "print(\"Article after removing stopwords  :   \",  ' '.join(lst))\n"
      ],
      "metadata": {
        "id": "g6ftvUSAXHl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4e7384-fec4-4e04-84ac-18be4451a748"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Article after removing stopwords  :    remove stopwords library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPGNi93wXx1w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**stopwords nltk**"
      ],
      "metadata": {
        "id": "V0gXCtPZXyLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "7eo4bLGnX5rd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words=stopwords.words('english')\n",
        "print(len(stop_words))\n",
        "print(stop_words)\n"
      ],
      "metadata": {
        "id": "R1M1la8RZTxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00410e6-8dbe-4127-f9e9-3b4cc40e93a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "article=\"we will show how to remove stopwords using spacy library\"\n",
        "print('Original Article: %s' % (article))\n",
        "print()"
      ],
      "metadata": {
        "id": "Lwqwr8g7YIWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daff5b26-30df-4b7d-a78e-695c23e1f756"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Article: we will show how to remove stopwords using spacy library\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "word_tokens=nltk.word_tokenize(article)\n",
        "print(word_tokens)\n",
        "print()"
      ],
      "metadata": {
        "id": "Sy2TbKXvYswY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c43c257-a4d5-4199-8bf9-6a5c44f654b3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', 'will', 'show', 'how', 'to', 'remove', 'stopwords', 'using', 'spacy', 'library']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##The lst will keep only those words that are not in stopwords\n",
        "lst=[]\n",
        "for token in word_tokens:\n",
        "    if token.lower() not in stop_words:    #checking whether the word is not \n",
        "        lst.append(token)                    #present in the stopword list.\n",
        "print()\n",
        "#Join items in the list\n",
        "print(\"Article after removing stopwords  :   \", lst)"
      ],
      "metadata": {
        "id": "_vhxG5ihZFl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e60d4b9-634f-424d-e0a6-5db756260f98"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Article after removing stopwords  :    ['show', 'remove', 'stopwords', 'using', 'spacy', 'library']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XAyOhGBZZ63a"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatize in spacy**"
      ],
      "metadata": {
        "id": "AV3TrdOKZ75r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spaCy lemmatizing\n",
        "import spacy\n",
        "#loading the english language small model of spacy\n",
        "\n",
        "spacy_nlp = spacy.load('en_core_web_sm')  \n",
        "article = spacy_nlp(\"we will show how to remove stopwords using spacy library \")\n"
      ],
      "metadata": {
        "id": "1XexneJ1Z66w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479981f0-955c-4dcb-fd84-7106cc66be2e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##geting tokens\n",
        "\n",
        "doc = spacy_nlp(article)\n",
        "tokens = [token.text for token in doc]\n",
        "print('Original Article: %s' % (article))\n",
        "print()\n",
        "print('Tokens:',tokens)\n"
      ],
      "metadata": {
        "id": "oX0ILi_9aPQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e762d99-0611-402f-dbe3-9fb3fd367158"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Article: we will show how to remove stopwords using spacy library \n",
            "\n",
            "Tokens: ['we', 'will', 'show', 'how', 'to', 'remove', 'stopwords', 'using', 'spacy', 'library']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_list = []\n",
        "for token in doc:\n",
        "    lemma_list.append(token.lemma_)\n",
        "print(\"Tokenize+Lemmatize:\")\n",
        "print(lemma_list)"
      ],
      "metadata": {
        "id": "CzWLBaY1aS4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df26026-f77d-48c9-abac-bcce03b7c127"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize+Lemmatize:\n",
            "['we', 'will', 'show', 'how', 'to', 'remove', 'stopword', 'use', 'spacy', 'library']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtnSIgUKalfe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmas in spacy**"
      ],
      "metadata": {
        "id": "DYIv_oeQamM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##lemma attributes on tokens in Spacy\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "conference_help_text = (\n",
        "\"In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer,[1] or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth.\"\n",
        ")\n",
        "conference_help_doc = nlp(conference_help_text)\n",
        "for token in conference_help_doc:\n",
        "    if str(token) != str(token.lemma_):\n",
        "         print(f\"{str(token):>20} : {str(token.lemma_)}\")"
      ],
      "metadata": {
        "id": "jG_2ye_uali4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1ea836-19f7-4c13-cf38-0978a82cbcbb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  In : in\n",
            "                  is : be\n",
            "          converting : convert\n",
            "          characters : character\n",
            "              tokens : token\n",
            "             strings : string\n",
            "            assigned : assign\n",
            "          identified : identify\n",
            "                   A : a\n",
            "            performs : perform\n",
            "              termed : term\n",
            "                  is : be\n",
            "                   A : a\n",
            "                  is : be\n",
            "            combined : combine\n",
            "           languages : language\n",
            "               pages : page\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xn_h1QfRa-pC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**lemmatize in nltk**"
      ],
      "metadata": {
        "id": "sa5m2111a-Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EuyZTaVTbBwN"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}